{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is data scraped and filtered from https://www.politifact.com.\n",
    "# The framework used here for deep learning with NLP comes from Dipanjan Sarkar's \"Text Analytics with Python\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install if needed\n",
    "# !pip install tensorflow==1.13.1\n",
    "# !pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essential dependencies\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:5096: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "path = os.path.abspath('politifact_post_eda')\n",
    "df = pd.read_csv(path, index_col=0)\n",
    "\n",
    "# filter to only what we will be using\n",
    "poli_df = df[[\"statement\",\"veracity\"]]\n",
    "\n",
    "# transform to binary classification data\n",
    "poli_df.veracity = poli_df.veracity.map(\n",
    "    {'False': False, 'True': True, 'Pants on Fire!': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checks if tensorflow will be using a GPU\n",
    "print(tf.test.is_gpu_available())\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1455 entries, 18 to 16610\n",
      "Data columns (total 2 columns):\n",
      "statement    1455 non-null object\n",
      "veracity     1455 non-null bool\n",
      "dtypes: bool(1), object(1)\n",
      "memory usage: 24.2+ KB\n"
     ]
    }
   ],
   "source": [
    "poli_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>veracity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\"The vast majority of Wisconsin students canno...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>\"President Trump has sent 14,000 American troo...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>\"To be clear, I’m not talking about confiscati...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>\"When my father became commander in chief of t...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>\"Ohio, Michigan, and Pennsylvania actually in ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            statement  veracity\n",
       "18  \"The vast majority of Wisconsin students canno...     False\n",
       "21  \"President Trump has sent 14,000 American troo...      True\n",
       "27  \"To be clear, I’m not talking about confiscati...     False\n",
       "30  \"When my father became commander in chief of t...     False\n",
       "42  \"Ohio, Michigan, and Pennsylvania actually in ...     False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poli_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing functions\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text,\"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe','script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+','\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode(\n",
    "        'ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def preprocess_document(document):\n",
    "    # strip HTML tags using function above\n",
    "    document = strip_html_tags(document)\n",
    "    # remove accented characters using function above\n",
    "    document = remove_accented_chars(document)\n",
    "    # lowercase\n",
    "    document = document.lower()\n",
    "    # remove extra whitespace\n",
    "    document = re.sub(' +', ' ', document)\n",
    "    document = document.strip()\n",
    "    # finished\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize for deep learning\n",
    "preprocess_corpus = np.vectorize(preprocess_document)\n",
    "\n",
    "# generate arrays\n",
    "statements = preprocess_corpus(poli_df.statement.values)\n",
    "veracity = poli_df.veracity.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((874,), (145,), (436,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's do a manual train/test/split as Dipanjan Sarkar does in his book (note you can also use train_test_split)\n",
    "# 1445 rows (train = 60% -> 874, validation = 10% -> 145, test = 30% -> 436; some numbers were rounded)\n",
    "train_statements = statements[:874]\n",
    "train_veracity = veracity[:874]\n",
    "\n",
    "validation_statements = statements[874:1019]\n",
    "validation_veracity = veracity[874:1019]\n",
    "\n",
    "test_statements = statements[1019:1455]\n",
    "test_veracity = veracity[1019:1455]\n",
    "\n",
    "train_statements.shape, validation_veracity.shape, test_statements.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build data ingestion functions and feature engineering pipelines\n",
    "# training input on the whole training set with no limits on training epochs\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    {'sentence': train_statements}, \n",
    "                    train_veracity,\n",
    "                    batch_size = 256, num_epochs = None, shuffle = True)\n",
    "\n",
    "# prediction on the whole training set\n",
    "predict_train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                            {'sentence': train_statements}, \n",
    "                            train_veracity,\n",
    "                            shuffle = False)\n",
    "\n",
    "# prediction on the whole validation set\n",
    "predict_validation_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                            {'sentence': validation_statements}, \n",
    "                            validation_veracity,\n",
    "                            shuffle = False)\n",
    "\n",
    "# prediction on the test set\n",
    "predict_test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                            {'sentence': test_statements}, \n",
    "                            test_veracity,\n",
    "                            shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "# define the sentence-embedding features that leverages Google's Universal Sentence Encoder\n",
    "# takes about 20 minutes to load; as of 2019/11/13 tensorflow version 2+ and encoder 3 do not work with this setup\n",
    "embedding_feature = hub.text_embedding_column(\n",
    "                    key = 'sentence', \n",
    "                    module_spec = \"https://tfhub.dev/google/universal-sentence-encoder/2\",\n",
    "                    trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/lx/dgklbb3d721cj2ghzkdtfn100000gn/T/tmpapaxg_6l\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/lx/dgklbb3d721cj2ghzkdtfn100000gn/T/tmpapaxg_6l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/lx/dgklbb3d721cj2ghzkdtfn100000gn/T/tmpapaxg_6l', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0xb2c890eb8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/lx/dgklbb3d721cj2ghzkdtfn100000gn/T/tmpapaxg_6l', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0xb2c890eb8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# simple feed-forward DNN with 2 hidden layers as we are just seeing how well a simple deep learning model performs\n",
    "\n",
    "dnn = tf.estimator.DNNClassifier(\n",
    "    hidden_units = [512, 128], \n",
    "    feature_columns = [embedding_feature],\n",
    "    n_classes = 2,\n",
    "    activation_fn = tf.nn.relu,\n",
    "    dropout = 0.1,\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate = 0.005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 0\n",
      "Train Time (s): 60.506800174713135\n",
      "Eval Metrics (Train): {'accuracy': 0.97940505, 'accuracy_baseline': 0.6441648, 'auc': 0.99756986, 'auc_precision_recall': 0.9944781, 'average_loss': 0.14703967, 'label/mean': 0.35583523, 'loss': 18.358952, 'precision': 0.96507937, 'prediction/mean': 0.36793247, 'recall': 0.977492, 'global_step': 200}\n",
      "Eval Metrics (Validation): {'accuracy': 0.6068966, 'accuracy_baseline': 0.5103448, 'auc': 0.66673017, 'auc_precision_recall': 0.6634409, 'average_loss': 0.7654497, 'label/mean': 0.5103448, 'loss': 55.495102, 'precision': 0.6164383, 'prediction/mean': 0.4957144, 'recall': 0.6081081, 'global_step': 200}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 100\n",
      "Train Time (s): 60.60412907600403\n",
      "Eval Metrics (Train): {'accuracy': 1.0, 'accuracy_baseline': 0.6441648, 'auc': 1.0, 'auc_precision_recall': 1.0, 'average_loss': 0.056800112, 'label/mean': 0.35583523, 'loss': 7.0919, 'precision': 1.0, 'prediction/mean': 0.34785253, 'recall': 1.0, 'global_step': 300}\n",
      "Eval Metrics (Validation): {'accuracy': 0.62068963, 'accuracy_baseline': 0.5103448, 'auc': 0.66063946, 'auc_precision_recall': 0.6676463, 'average_loss': 0.8979169, 'label/mean': 0.5103448, 'loss': 65.098976, 'precision': 0.64179105, 'prediction/mean': 0.43868735, 'recall': 0.5810811, 'global_step': 300}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 200\n",
      "Train Time (s): 59.1946759223938\n",
      "Eval Metrics (Train): {'accuracy': 1.0, 'accuracy_baseline': 0.6441648, 'auc': 1.0, 'auc_precision_recall': 0.99999994, 'average_loss': 0.027026601, 'label/mean': 0.35583523, 'loss': 3.3744643, 'precision': 1.0, 'prediction/mean': 0.35309827, 'recall': 1.0, 'global_step': 400}\n",
      "Eval Metrics (Validation): {'accuracy': 0.6068966, 'accuracy_baseline': 0.5103448, 'auc': 0.6530263, 'auc_precision_recall': 0.6618516, 'average_loss': 0.9998914, 'label/mean': 0.5103448, 'loss': 72.49213, 'precision': 0.6231884, 'prediction/mean': 0.45211208, 'recall': 0.5810811, 'global_step': 400}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 300\n",
      "Train Time (s): 59.82261919975281\n",
      "Eval Metrics (Train): {'accuracy': 1.0, 'accuracy_baseline': 0.6441648, 'auc': 1.0, 'auc_precision_recall': 1.0, 'average_loss': 0.015779937, 'label/mean': 0.35583523, 'loss': 1.9702377, 'precision': 1.0, 'prediction/mean': 0.35725784, 'recall': 1.0, 'global_step': 500}\n",
      "Eval Metrics (Validation): {'accuracy': 0.59310347, 'accuracy_baseline': 0.5103448, 'auc': 0.6474115, 'auc_precision_recall': 0.6566968, 'average_loss': 1.0767939, 'label/mean': 0.5103448, 'loss': 78.06756, 'precision': 0.6, 'prediction/mean': 0.4831996, 'recall': 0.6081081, 'global_step': 500}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 400\n",
      "Train Time (s): 58.65150809288025\n",
      "Eval Metrics (Train): {'accuracy': 1.0, 'accuracy_baseline': 0.6441648, 'auc': 1.0, 'auc_precision_recall': 1.0, 'average_loss': 0.01055556, 'label/mean': 0.35583523, 'loss': 1.317937, 'precision': 1.0, 'prediction/mean': 0.35753682, 'recall': 1.0, 'global_step': 600}\n",
      "Eval Metrics (Validation): {'accuracy': 0.6, 'accuracy_baseline': 0.5103448, 'auc': 0.64741147, 'auc_precision_recall': 0.64970136, 'average_loss': 1.1420528, 'label/mean': 0.5103448, 'loss': 82.79883, 'precision': 0.6052632, 'prediction/mean': 0.49189332, 'recall': 0.6216216, 'global_step': 600}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 500\n",
      "Train Time (s): 60.80056285858154\n",
      "Eval Metrics (Train): {'accuracy': 1.0, 'accuracy_baseline': 0.6441648, 'auc': 0.99999994, 'auc_precision_recall': 1.0, 'average_loss': 0.0074055954, 'label/mean': 0.35583523, 'loss': 0.9246415, 'precision': 1.0, 'prediction/mean': 0.35583776, 'recall': 1.0, 'global_step': 700}\n",
      "Eval Metrics (Validation): {'accuracy': 0.6, 'accuracy_baseline': 0.5103448, 'auc': 0.6443662, 'auc_precision_recall': 0.64619195, 'average_loss': 1.2010843, 'label/mean': 0.5103448, 'loss': 87.078606, 'precision': 0.6052632, 'prediction/mean': 0.4759584, 'recall': 0.6216216, 'global_step': 700}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 600\n",
      "Train Time (s): 60.891327142715454\n",
      "Eval Metrics (Train): {'accuracy': 1.0, 'accuracy_baseline': 0.6441648, 'auc': 0.99999994, 'auc_precision_recall': 1.0, 'average_loss': 0.005621356, 'label/mean': 0.35583523, 'loss': 0.70186645, 'precision': 1.0, 'prediction/mean': 0.35573182, 'recall': 1.0, 'global_step': 800}\n",
      "Eval Metrics (Validation): {'accuracy': 0.6068966, 'accuracy_baseline': 0.5103448, 'auc': 0.6445566, 'auc_precision_recall': 0.6476545, 'average_loss': 1.2435241, 'label/mean': 0.5103448, 'loss': 90.155495, 'precision': 0.61333334, 'prediction/mean': 0.47817045, 'recall': 0.6216216, 'global_step': 800}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 700\n",
      "Train Time (s): 60.63615417480469\n",
      "Eval Metrics (Train): {'accuracy': 1.0, 'accuracy_baseline': 0.6441648, 'auc': 1.0, 'auc_precision_recall': 1.0, 'average_loss': 0.0044402317, 'label/mean': 0.35583523, 'loss': 0.55439466, 'precision': 1.0, 'prediction/mean': 0.35571918, 'recall': 1.0, 'global_step': 900}\n",
      "Eval Metrics (Validation): {'accuracy': 0.6068966, 'accuracy_baseline': 0.5103448, 'auc': 0.64389044, 'auc_precision_recall': 0.64565015, 'average_loss': 1.2767111, 'label/mean': 0.5103448, 'loss': 92.561554, 'precision': 0.61333334, 'prediction/mean': 0.478182, 'recall': 0.6216216, 'global_step': 900}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 800\n",
      "Train Time (s): 57.362359046936035\n",
      "Eval Metrics (Train): {'accuracy': 1.0, 'accuracy_baseline': 0.6441648, 'auc': 1.0, 'auc_precision_recall': 1.0, 'average_loss': 0.0036245536, 'label/mean': 0.35583523, 'loss': 0.4525514, 'precision': 1.0, 'prediction/mean': 0.3555925, 'recall': 1.0, 'global_step': 1000}\n",
      "Eval Metrics (Validation): {'accuracy': 0.6137931, 'accuracy_baseline': 0.5103448, 'auc': 0.6386563, 'auc_precision_recall': 0.6373849, 'average_loss': 1.31524, 'label/mean': 0.5103448, 'loss': 95.354904, 'precision': 0.6216216, 'prediction/mean': 0.47513327, 'recall': 0.6216216, 'global_step': 1000}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 900\n",
      "Train Time (s): 58.46832489967346\n",
      "Eval Metrics (Train): {'accuracy': 1.0, 'accuracy_baseline': 0.6441648, 'auc': 1.0, 'auc_precision_recall': 1.0, 'average_loss': 0.0030193122, 'label/mean': 0.35583523, 'loss': 0.3769827, 'precision': 1.0, 'prediction/mean': 0.35577783, 'recall': 1.0, 'global_step': 1100}\n",
      "Eval Metrics (Validation): {'accuracy': 0.6137931, 'accuracy_baseline': 0.5103448, 'auc': 0.637895, 'auc_precision_recall': 0.63687694, 'average_loss': 1.3436689, 'label/mean': 0.5103448, 'loss': 97.416, 'precision': 0.6184211, 'prediction/mean': 0.481927, 'recall': 0.6351351, 'global_step': 1100}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1000\n",
      "Train Time (s): 61.13609600067139\n",
      "Eval Metrics (Train): {'accuracy': 1.0, 'accuracy_baseline': 0.6441648, 'auc': 1.0, 'auc_precision_recall': 1.0, 'average_loss': 0.002568386, 'label/mean': 0.35583523, 'loss': 0.32068133, 'precision': 1.0, 'prediction/mean': 0.35571775, 'recall': 1.0, 'global_step': 1200}\n",
      "Eval Metrics (Validation): {'accuracy': 0.62068963, 'accuracy_baseline': 0.5103448, 'auc': 0.63532543, 'auc_precision_recall': 0.6354723, 'average_loss': 1.3733491, 'label/mean': 0.5103448, 'loss': 99.56781, 'precision': 0.62666667, 'prediction/mean': 0.48015767, 'recall': 0.6351351, 'global_step': 1200}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Time (s): 61.869592905044556\n",
      "Eval Metrics (Train): {'accuracy': 1.0, 'accuracy_baseline': 0.6441648, 'auc': 1.0, 'auc_precision_recall': 1.0, 'average_loss': 0.0022163682, 'label/mean': 0.35583523, 'loss': 0.2767294, 'precision': 1.0, 'prediction/mean': 0.3557735, 'recall': 1.0, 'global_step': 1300}\n",
      "Eval Metrics (Validation): {'accuracy': 0.62068963, 'accuracy_baseline': 0.5103448, 'auc': 0.6379901, 'auc_precision_recall': 0.6377767, 'average_loss': 1.3933696, 'label/mean': 0.5103448, 'loss': 101.019295, 'precision': 0.62666667, 'prediction/mean': 0.48358113, 'recall': 0.6351351, 'global_step': 1300}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1200\n",
      "Train Time (s): 65.16779971122742\n",
      "Eval Metrics (Train): {'accuracy': 1.0, 'accuracy_baseline': 0.6441648, 'auc': 1.0, 'auc_precision_recall': 1.0, 'average_loss': 0.0019411017, 'label/mean': 0.35583523, 'loss': 0.24236043, 'precision': 1.0, 'prediction/mean': 0.3559188, 'recall': 1.0, 'global_step': 1400}\n",
      "Eval Metrics (Validation): {'accuracy': 0.62068963, 'accuracy_baseline': 0.5103448, 'auc': 0.63675296, 'auc_precision_recall': 0.63743585, 'average_loss': 1.4130069, 'label/mean': 0.5103448, 'loss': 102.443, 'precision': 0.6233766, 'prediction/mean': 0.49050957, 'recall': 0.6486486, 'global_step': 1400}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1300\n",
      "Train Time (s): 64.42940616607666\n",
      "Eval Metrics (Train): {'accuracy': 1.0, 'accuracy_baseline': 0.6441648, 'auc': 1.0, 'auc_precision_recall': 1.0, 'average_loss': 0.0017105081, 'label/mean': 0.35583523, 'loss': 0.21356916, 'precision': 1.0, 'prediction/mean': 0.35585606, 'recall': 1.0, 'global_step': 1500}\n",
      "Eval Metrics (Validation): {'accuracy': 0.6275862, 'accuracy_baseline': 0.5103448, 'auc': 0.63932246, 'auc_precision_recall': 0.638424, 'average_loss': 1.4306111, 'label/mean': 0.5103448, 'loss': 103.71931, 'precision': 0.6315789, 'prediction/mean': 0.48831546, 'recall': 0.6486486, 'global_step': 1500}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1400\n",
      "Train Time (s): 57.573789834976196\n",
      "Eval Metrics (Train): {'accuracy': 1.0, 'accuracy_baseline': 0.6441648, 'auc': 1.0, 'auc_precision_recall': 1.0, 'average_loss': 0.0015248536, 'label/mean': 0.35583523, 'loss': 0.19038887, 'precision': 1.0, 'prediction/mean': 0.35583252, 'recall': 1.0, 'global_step': 1600}\n",
      "Eval Metrics (Validation): {'accuracy': 0.6275862, 'accuracy_baseline': 0.5103448, 'auc': 0.6385611, 'auc_precision_recall': 0.6407598, 'average_loss': 1.4520675, 'label/mean': 0.5103448, 'loss': 105.274895, 'precision': 0.6315789, 'prediction/mean': 0.48687634, 'recall': 0.6486486, 'global_step': 1600}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 1500\n",
      "Train Time (s): 59.293241024017334\n",
      "Eval Metrics (Train): {'accuracy': 1.0, 'accuracy_baseline': 0.6441648, 'auc': 1.0, 'auc_precision_recall': 1.0, 'average_loss': 0.0013723333, 'label/mean': 0.35583523, 'loss': 0.1713456, 'precision': 1.0, 'prediction/mean': 0.35584152, 'recall': 1.0, 'global_step': 1700}\n",
      "Eval Metrics (Validation): {'accuracy': 0.6275862, 'accuracy_baseline': 0.5103448, 'auc': 0.6404644, 'auc_precision_recall': 0.6464345, 'average_loss': 1.4702237, 'label/mean': 0.5103448, 'loss': 106.59122, 'precision': 0.6315789, 'prediction/mean': 0.48704466, 'recall': 0.6486486, 'global_step': 1700}\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "TOTAL_STEPS = 1500\n",
    "STEP_SIZE = 100\n",
    "for step in range(0, TOTAL_STEPS+1, STEP_SIZE):\n",
    "    print()\n",
    "    print('-'*100)\n",
    "    print('Training for step =', step)\n",
    "    start_time = time.time()\n",
    "    dnn.train(input_fn = train_input_fn, steps = STEP_SIZE)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('Train Time (s):', elapsed_time)\n",
    "    print(\"\")\n",
    "    print('Eval Metrics (Train):', dnn.evaluate(input_fn = predict_train_input_fn))\n",
    "    print(\"\")\n",
    "    print('Eval Metrics (Validation):', dnn.evaluate(input_fn = predict_validation_input_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 1.0,\n",
       " 'accuracy_baseline': 0.6441648,\n",
       " 'auc': 1.0,\n",
       " 'auc_precision_recall': 1.0,\n",
       " 'average_loss': 0.0013723333,\n",
       " 'label/mean': 0.35583523,\n",
       " 'loss': 0.1713456,\n",
       " 'precision': 1.0,\n",
       " 'prediction/mean': 0.35584152,\n",
       " 'recall': 1.0,\n",
       " 'global_step': 1700}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn.evaluate(input_fn = predict_train_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.61009175,\n",
       " 'accuracy_baseline': 0.57339454,\n",
       " 'auc': 0.6300968,\n",
       " 'auc_precision_recall': 0.58227587,\n",
       " 'average_loss': 1.650802,\n",
       " 'label/mean': 0.4266055,\n",
       " 'loss': 179.93742,\n",
       " 'precision': 0.5555556,\n",
       " 'prediction/mean': 0.33982703,\n",
       " 'recall': 0.43010753,\n",
       " 'global_step': 1700}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn.evaluate(input_fn = predict_test_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the model was able to predict about 4% better than the baseline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
